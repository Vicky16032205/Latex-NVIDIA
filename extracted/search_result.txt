{'query': 'Convolutional neural network', 'user_based_search': {'cnn1': [{'text': "As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels.\n\nThese kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.\n\nAs we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that 'fire' when they see a specific feature at a given spatial position of the input. These are commonly known as activations.\n\n| Input Vector | | | | | | Pooled Vector | | | Kernel | | | Destination Pixel |\n| - | - | - | - | - | - | - | - | - | - | - | - | - |\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | -8 |\n| 0 | 1 | 2 | 1 | 1 | 2 | 0 | 1 | 2 | 0 | 0 | 0 | |\n| 0 | 1 | 1 | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | -4 | |\n| 1 | 0 | 0 | 0 | 0 | 0 | | | | | | | |\n| 0 | 0 | 1 | 1 | 1 | 0 | | | | | | | |\n| 0 | 1 | 1 | 1 | 1 | 1 | | | | | | | |\n\n\nFig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.\n\nEvery kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.\n\nAs we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fully-connected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.\n\nFor example, if the input to the network is an image of size 64 × 64 × 3 (a RGB-coloured image with a dimensionality of 64 × 64) and we set the receptive field size as 6 × 6, we would have a total of 108 weights on each neuron within the convolutional layer. (6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12,288 weights each.\n\nConvolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth, the stride and setting zero-padding.\n\nIntroduction to Convolutional Neural Networks    7\n\nThe depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.\n\nWe are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.\n\nZero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.\n\nIt is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:\n\n$$(V - R) + 2Z \\over S + 1$$\n\nWhere V represents the input volume size (height×width×depth), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.\n\nDespite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.\n\nParameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.\n\nAs a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.\n\n8      Keiron O'Shea et al.", 'sub_heading': '2.2 Convolutional layer', 'collection_name': 'cnn1', 'similarity': 0.8848913908004761}, {'text': "As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.\n\n4      Keiron O'Shea et al.\n\nOne of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input (height and the width) and the depth. The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.\n\nIn practice this would mean that for the example given earlier, the input 'volume' will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.", 'sub_heading': '2 CNN architecture', 'collection_name': 'cnn1', 'similarity': 0.835577130317688}], 'cnn2': [{'text': 'Convolutional Neural Networks (CNNs), introduced by Le Cun et al. [6] are a class of biologically inspired neural networks which solve equation (1) by passing X through a series of convolutional filters and simple non-linearities. They have shown remarkable results in a wide variety of machine learning problems [8]. Figure 1 shows a typical CNN architecture.\n\nA convolutional neural network has a hierarchical architecture. Starting from the input signal x, each subsequent layer x_j is computed as\n\n$$x_j = ρW_jx_{j-1} \\quad (5)$$\n\nHere W_j is a linear operator and ρ is a non-linearity. Typically, in a CNN, W_j is a convolution, and ρ is a rectifier max(x, 0) or sigmoid 1/(1+exp(−x)). It is easier to think of the operator W_j as a stack of convolutional filters. So the layers are filter maps and each layer can be written as a sum of convolutions of the previous layer.\n\n$$x_j(u, k_j) = ρ(\\sum_k (x_{j-1}(., k) * W_{j,k_j}(., k))(u)) \\quad (6)$$\n\nHere * is the discrete convolution operator:\n\n$$(f * g)(x) = \\sum_{u=-\\infty}^{\\infty} f(u)g(x - u) \\quad (7)$$\n\nThe optimization problem defined by a convolutional neural network is highly non-convex. So typically, the weights W_j are learned by stochastic gradient descent, using the backpropagation algorithm to compute gradients.\n\nFigure 1: Architecture of a Convolutional Neural Network (from LeCun et al. [7])\n\n| INPUT 32x32 | C1: feature maps 6\\@28x28 | C3: f. maps 16\\@10x10 | S4: f. maps 16\\@5x5 | C5: layer 120 | F6: layer 84 | OUTPUT 10 |\n| - | - | - | - | - | - | - |\n| Convolutions | | Subsampling | Convolutions | Subsampling | Full connection | Gaussian connections |\n| | | | | | | Full connection |', 'sub_heading': '1.5 Convolutional Neural Networks', 'collection_name': 'cnn2', 'similarity': 0.9026689529418945}, {'text': 'The scattering transform described in the previous section provides a simple view of a general convolutional neural netowrk. While it provides intuition behind the working of CNNs, the transformation suffers from high variance and loss of information because we only consider single channel convolutions. To analyze the properties of general CNN architectures, we must allow for channel combinations. Mallat [10] extends previously introduced tools to develop a mathematical framework for this analysis. The theory is, however, out of the scope of this paper. At a high level, the extension is achieved by replacing the requirement of contractions and invariants to translations by contractions along adaptive groups of local symmetries. Further, the wavelets are replaced by adapted filter weights similar to deep learning models.', 'sub_heading': '5 General Convolutional Neural Network Architectures', 'collection_name': 'cnn2', 'similarity': 0.8422203063964844}, {'text': 'Mallat [10] introduced a mathematical framework for analyzing the properties of convolutional networks. The theory is based on extensive prior work on wavelet scattering (see for example [2, 1]) and illustrates that to compute invariants, we must separate variations of X at different scales with a wavelet transform. The theory is a first step towards understanding general classes of CNNs, and this paper presents its key concepts.', 'sub_heading': '1.6 A mathematical framework for CNNs', 'collection_name': 'cnn2', 'similarity': 0.8173869848251343}, {'text': 'The WFT fails because it introduces scale (the width of the window) into the analysis. The continuous wavelet transform involves scale too, but it considers all possible scalings and avoids the problem faced by the WFT. Again, we begin with a window function ψ (supported on [−T, 0]), this time called a mother wavelet. For some fixed p ≥ 0, we define\n\n$$ψ_s(u) ≡ |s|^{-p}ψ(\\frac{u}{s})$$\n(10)\n\nThe scale s is allowed to be any non-zero real number. With this family of wavelets, we define the continuous wavelet transform (CWT) as\n\n$$\\tilde{f}(s, t) ≡ (f * ψ_s)(t)$$\n(11)\n\nwhere * is the continuous convolution operator:\n\n$$(p * q)(x) ≡ \\int_{-\\infty}^{\\infty} p(u)q(x - u)du$$\n(12)\n\nThe continuous wavelet transform captures variations in f at a particular scale. It provides the foundation for the operation of CNNs, as will be explored next.', 'sub_heading': '2.3 Continuous wavelet transform', 'collection_name': 'cnn2', 'similarity': 0.8103387951850891}], 'cnn3': [], 'cnn4': []}, 'default_results': {'Introduction': {'cnn1': [{'text': '1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642–3649. IEEE (2012)\n\n2. Cireşan, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2013, pp. 411–418. Springer (2013)\n\n3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\n\nIntroduction to Convolutional Neural Networks         11\n\n4. Cireşan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE (2011)\n\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networks a review. Pattern recognition 35(10), 2279–2301 (2002)\n\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257–260. IEEE (2010)\n\n7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\n\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\n\n9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221–231 (2013)\n\n10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725–1732. IEEE (2014)\n\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097–1105 (2012)\n\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541–551 (1989)\n\n13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n\n14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685–696 (1998)\n\n15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\n\n16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\n\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224–229. IEEE (2005)\n\n18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553–2561 (2013)\n\n19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157–2162. IEEE (2003)\n\n20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\n\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision–ECCV 2014, pp. 818–833. Springer (2014)', 'similarity': 0.856539249420166}], 'cnn2': [{'text': '', 'similarity': 0.9304435849189758}], 'cnn3': [{'text': 'In order to investigate the effect of the goodness of the first convolutional layer, we train the CNN configurations reported in Section 3.1 again, but this time including the first layer in the goodness computation. Figure 9, reports the results, highlighting that the training of the first layer affects the speed of convergence of the next layers. Adding the first layer also reduces the overall accuracy of the network by approximately 2%.\n\nSCODELLARO, KULKARNI, ALVES AND SCHRÖTER\n\nChart showing accuracy over epochs for different layers and training conditions\n\nFigure 9: Our implementation of FF trained CNNs does not require the inclusion of the goodness of the first layer during training. Continuous lines represent evolution of the discrimination accuracy during the training phase, when the first layer is not included. Dashed lines represent the discrimination accuracy evolution if its goodness is included.', 'similarity': 0.8171168565750122}], 'cnn4': [{'text': 'Convolutional Neural Networks (CNNs) have revolutionized the field of deep learning, especially in processing grid-like data structures such as images [1]. Their effectiveness in tasks like image classification [2, 3], object detection [4, 5], semantic segmentation [6, 7] and image generation [8] stem from their ability to effectively learn spatial features. Convolutional layers, using filters or kernels, capture local patterns and extract features from input images. One important feature of convolutional layers is the shared weights implemented by kernels. This allows for efficient deep learning on images, as using only fully connected layers for such tasks would result in unfathomable numbers of parameters. Pooling layers, like max pooling and average pooling, reduce the spatial dimensions of these features, helping the network to focus on the most significant aspects.\n\nDespite their popularity, CNNs face challenges in computational efficiency and adaptability. There have been several convolutional neural network architectures that have been proposed that are aimed at efficiency. Some of such architectures include MobileNet [12] and EfficientNet [13]. However, such traditional CNNs, with fixed architectures and number of parameters, may not perform uniformly across different types of input data with varying levels of complexity.\n\nNeural Architecture Search (NAS), a method for selecting optimal neural network architectures, has been a response to this challenge. NAS aims to obtain the best model for a specific task under certain constraints [14]. However, NAS is often resource-intensive due to the need to train multiple candidate models. in order to determine the optimal architecture. It is estimated that the carbon emission produced when using NAS to train a transformer model can amount to five times the lifetime carbon emissions of an average car [15]. This highlights the importance of finding suitable\n\narchitectures for neural networks, yet also points to the limitations of current approaches in terms of static structure and proneness to over-parameterization.\n\nSelf Expanding Neural Networks (SENN), introduced in [9], offer a promising direction. Inspired by neurogenesis, SENN dynamically adds neurons and fully connected layers to the architecture during training using a natural expansion score (defined in section 2.1) as a criteria to guide this process. This helps overcome the problem of over-parametrization. However, its application has been limited to multilayer perceptrons, with extensions to more practical architectures like CNNs identified as a future research prospect.\n\nOur study aims to develop a Self Expanding Convolutional Neural Network (SECNN), building on the concept of SENN and applying it to modern vision tasks. To the best of our knowledge, there has been no research on Self Expanding CNNs, despite the potential they hold for addressing model efficiency and adaptability in vision tasks. Unlike existing approaches that often require restarting training after modifications or rely on preset mechanisms for expansion, our approach utilizes the natural expansion score for dynamic and optimal model expansion. This research represents a significant step in developing adaptable, efficient CNN models for a variety of vision-related tasks.\n\nThe contributions of this research are as follows:\n\n- Developing a Self Expanding CNN that dynamically determines the optimal model size based on the task, thereby enhancing efficiency.\n- Eliminating the need to train multiple CNN models of varying sizes by allowing for the extraction of checkpoints at diverse complexity levels.\n- Eliminating the need to restart the training process after expanding the CNN model.', 'similarity': 0.9304435849189758}]}, 'Abstract': {'cnn1': [{'text': '1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642–3649. IEEE (2012)\n\n2. Cireşan, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2013, pp. 411–418. Springer (2013)\n\n3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\n\nIntroduction to Convolutional Neural Networks         11\n\n4. Cireşan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE (2011)\n\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networks a review. Pattern recognition 35(10), 2279–2301 (2002)\n\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257–260. IEEE (2010)\n\n7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\n\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\n\n9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221–231 (2013)\n\n10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725–1732. IEEE (2014)\n\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097–1105 (2012)\n\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541–551 (1989)\n\n13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n\n14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685–696 (1998)\n\n15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\n\n16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\n\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224–229. IEEE (2005)\n\n18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553–2561 (2013)\n\n19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157–2162. IEEE (2003)\n\n20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\n\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision–ECCV 2014, pp. 818–833. Springer (2014)', 'similarity': 0.8253905773162842}], 'cnn2': [{'text': '', 'similarity': 0.8390400409698486}], 'cnn3': [{'text': 'CNNs are considered the gold standard in deep learning-based image analysis. For instance, in biomedical imaging, they overcome the drawbacks of subjective analysis in the semi-quantitative visual inspection of samples (Gurcan et al., 2009), and they support experts\n\n\n\n\nFigure 6: Class activation maps (CAMs) of a FF trained CNN show which image regions are considered beneficial (yellow) or deleterious (pink) by the network for making its prediction. (A), (C), (E), and (G) display four input images. (B), (D), (F), and (H) are their corresponding CAMs. All examples are from a network with 16 convolutional neurons per layer, filter size 5x5, and trained with a batch size of 50.\n\n\n\nFigure 7: Class activation maps show that the different layers of the FF-trained CNN provide similar, but yet distinguishable information. (A) shows the CAM obtained from considering both layer 2 and layer 3 together. (B) and (C) show the CAMs obtained respectively only from layer 2 and layer 3.\n\nduring their daily clinical routine by reducing their workload (Shmatko et al., 2022). Furthermore, their exploitation of the spatial information within images makes them suitable for the deployment of explainable AI tools (such as class activation maps), which highlight the image regions contributing most significantly to the classification outcome. Our implementation of FF trained CNN shows that with the right choice of hyperparameters, this technique is competitive with backpropagation. These results were obtained without implementing all the possible and suggested optimizations such as enforcing symmetry of the loss function (Lee and Song, 2023) or choosing hard, i.e. easily confused, labels for the negative data set, as suggested by Hinton (2022). We propose that our work shows the\n\n13\n\nSCODELLARO, KULKARNI, ALVES AND SCHRÖTER\n\npotential of FF trained CNNs to address real world computer vision problems. An open question remains if this technique will supersede BP in specific applications. We believe that this potential exists, especially in the cases of neuromorphic hardware and unsupervised learning.\n\nA better understanding of the FF training will however also expand our understanding of the generic concept of neuronal information processing in all its breadth from biological systems to reservoir computing. The demonstrated capability to implement class activation maps offers an initial insight into these research topics. Achieving deeper insights will also mean to understand how the two innovations of FF, providing positive and negative labels and computing a locally defined goodness parameter, contribute to its success individually and synergetically (Tosato et al., 2023). Moreover, a better understanding why it is beneficial to exclude the first layer during the goodness computation (c.f. Appendix B) would be desirable. Subsequent work on FF training should also address its ability to train deeper networks, most likely expanding on the work of Lorberbom et al. (2023). Also the ability of FF training to work with larger and more complex data sets needs to be explored. Finally, its connection to biological neuronal systems (Ororbia and Mali, 2023; Ororbia, 2023) seems a promising research direction.', 'similarity': 0.807677686214447}], 'cnn4': [{'text': 'Convolutional Neural Networks (CNNs) have revolutionized the field of deep learning, especially in processing grid-like data structures such as images [1]. Their effectiveness in tasks like image classification [2, 3], object detection [4, 5], semantic segmentation [6, 7] and image generation [8] stem from their ability to effectively learn spatial features. Convolutional layers, using filters or kernels, capture local patterns and extract features from input images. One important feature of convolutional layers is the shared weights implemented by kernels. This allows for efficient deep learning on images, as using only fully connected layers for such tasks would result in unfathomable numbers of parameters. Pooling layers, like max pooling and average pooling, reduce the spatial dimensions of these features, helping the network to focus on the most significant aspects.\n\nDespite their popularity, CNNs face challenges in computational efficiency and adaptability. There have been several convolutional neural network architectures that have been proposed that are aimed at efficiency. Some of such architectures include MobileNet [12] and EfficientNet [13]. However, such traditional CNNs, with fixed architectures and number of parameters, may not perform uniformly across different types of input data with varying levels of complexity.\n\nNeural Architecture Search (NAS), a method for selecting optimal neural network architectures, has been a response to this challenge. NAS aims to obtain the best model for a specific task under certain constraints [14]. However, NAS is often resource-intensive due to the need to train multiple candidate models. in order to determine the optimal architecture. It is estimated that the carbon emission produced when using NAS to train a transformer model can amount to five times the lifetime carbon emissions of an average car [15]. This highlights the importance of finding suitable\n\narchitectures for neural networks, yet also points to the limitations of current approaches in terms of static structure and proneness to over-parameterization.\n\nSelf Expanding Neural Networks (SENN), introduced in [9], offer a promising direction. Inspired by neurogenesis, SENN dynamically adds neurons and fully connected layers to the architecture during training using a natural expansion score (defined in section 2.1) as a criteria to guide this process. This helps overcome the problem of over-parametrization. However, its application has been limited to multilayer perceptrons, with extensions to more practical architectures like CNNs identified as a future research prospect.\n\nOur study aims to develop a Self Expanding Convolutional Neural Network (SECNN), building on the concept of SENN and applying it to modern vision tasks. To the best of our knowledge, there has been no research on Self Expanding CNNs, despite the potential they hold for addressing model efficiency and adaptability in vision tasks. Unlike existing approaches that often require restarting training after modifications or rely on preset mechanisms for expansion, our approach utilizes the natural expansion score for dynamic and optimal model expansion. This research represents a significant step in developing adaptable, efficient CNN models for a variety of vision-related tasks.\n\nThe contributions of this research are as follows:\n\n- Developing a Self Expanding CNN that dynamically determines the optimal model size based on the task, thereby enhancing efficiency.\n- Eliminating the need to train multiple CNN models of varying sizes by allowing for the extraction of checkpoints at diverse complexity levels.\n- Eliminating the need to restart the training process after expanding the CNN model.', 'similarity': 0.8390400409698486}]}, 'Conclusion': {'cnn1': [{'text': 'Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.\n\nThis paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.\n\nResearch in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.', 'similarity': 0.9333658218383789}], 'cnn2': [{'text': 'In this paper, we tried to analyze the properties of convolutional neural networks. A simplified model, the scattering transform was introduced as a first step towards understanding CNN operations. We saw that the feature transformation is built on top of wavelet transforms which separate variations at different scales using a wavelet transform. The analysis of general CNN architectures was not considered in this paper, but even this analysis is only a first step towards a full mathematical understanding of convolutional neural networks.', 'similarity': 0.9318594932556152}], 'cnn3': [{'text': 'CNNs are considered the gold standard in deep learning-based image analysis. For instance, in biomedical imaging, they overcome the drawbacks of subjective analysis in the semi-quantitative visual inspection of samples (Gurcan et al., 2009), and they support experts\n\n\n\n\nFigure 6: Class activation maps (CAMs) of a FF trained CNN show which image regions are considered beneficial (yellow) or deleterious (pink) by the network for making its prediction. (A), (C), (E), and (G) display four input images. (B), (D), (F), and (H) are their corresponding CAMs. All examples are from a network with 16 convolutional neurons per layer, filter size 5x5, and trained with a batch size of 50.\n\n\n\nFigure 7: Class activation maps show that the different layers of the FF-trained CNN provide similar, but yet distinguishable information. (A) shows the CAM obtained from considering both layer 2 and layer 3 together. (B) and (C) show the CAMs obtained respectively only from layer 2 and layer 3.\n\nduring their daily clinical routine by reducing their workload (Shmatko et al., 2022). Furthermore, their exploitation of the spatial information within images makes them suitable for the deployment of explainable AI tools (such as class activation maps), which highlight the image regions contributing most significantly to the classification outcome. Our implementation of FF trained CNN shows that with the right choice of hyperparameters, this technique is competitive with backpropagation. These results were obtained without implementing all the possible and suggested optimizations such as enforcing symmetry of the loss function (Lee and Song, 2023) or choosing hard, i.e. easily confused, labels for the negative data set, as suggested by Hinton (2022). We propose that our work shows the\n\n13\n\nSCODELLARO, KULKARNI, ALVES AND SCHRÖTER\n\npotential of FF trained CNNs to address real world computer vision problems. An open question remains if this technique will supersede BP in specific applications. We believe that this potential exists, especially in the cases of neuromorphic hardware and unsupervised learning.\n\nA better understanding of the FF training will however also expand our understanding of the generic concept of neuronal information processing in all its breadth from biological systems to reservoir computing. The demonstrated capability to implement class activation maps offers an initial insight into these research topics. Achieving deeper insights will also mean to understand how the two innovations of FF, providing positive and negative labels and computing a locally defined goodness parameter, contribute to its success individually and synergetically (Tosato et al., 2023). Moreover, a better understanding why it is beneficial to exclude the first layer during the goodness computation (c.f. Appendix B) would be desirable. Subsequent work on FF training should also address its ability to train deeper networks, most likely expanding on the work of Lorberbom et al. (2023). Also the ability of FF training to work with larger and more complex data sets needs to be explored. Finally, its connection to biological neuronal systems (Ororbia and Mali, 2023; Ororbia, 2023) seems a promising research direction.', 'similarity': 0.8813095092773438}], 'cnn4': [{'text': 'In this article, we introduced the Self Expanding Convolutional Neural Network, a dynamically expanding architecture that uses the natural expansion score to optimize model growth. The CIFAR-10 dataset was used to train an initial model consisting over 5 different trials, which resulted in a 84.1% mean validation accuracy. Our model demonstrates how a Self Expanding CNN offers a computationally efficient solution to dynamically determine an optimal architecture for vision tasks while eliminating the need to restart or train multiple models.', 'similarity': 0.9318594932556152}]}, 'References': {'cnn1': [{'text': '1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642–3649. IEEE (2012)\n\n2. Cireşan, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2013, pp. 411–418. Springer (2013)\n\n3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\n\nIntroduction to Convolutional Neural Networks         11\n\n4. Cireşan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE (2011)\n\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networks a review. Pattern recognition 35(10), 2279–2301 (2002)\n\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257–260. IEEE (2010)\n\n7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\n\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\n\n9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221–231 (2013)\n\n10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725–1732. IEEE (2014)\n\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097–1105 (2012)\n\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541–551 (1989)\n\n13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n\n14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685–696 (1998)\n\n15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\n\n16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\n\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224–229. IEEE (2005)\n\n18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553–2561 (2013)\n\n19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157–2162. IEEE (2003)\n\n20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\n\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision–ECCV 2014, pp. 818–833. Springer (2014)', 'similarity': 1.0000001192092896}], 'cnn2': [{'text': '[1] Joakim Andén and Stéphane Mallat. Deep scattering spectrum. Signal Processing, IEEE Transactions on, 62(16):4114–4128, 2014.\n\n[2] Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1872–1886, 2013.\n\n[3] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3): 273–297, 1995.\n\n[4] Joan Bruna Estrach. Scattering representations for recognition.\n\n[5] Kaiser Gerald. A friendly guide to wavelets, 1994.\n\n[6] B Boser Le Cun, John S Denker, D Henderson, Richard E Howard, W Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems. Citeseer, 1990.\n\n[7] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\n[8] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.\n\n[9] Stéphane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):1331–1398, 2012.\n\n[10] Stéphane Mallat. Understanding deep convolutional networks. arXiv preprint arXiv:1601.04920, 2016.', 'similarity': 1.0000001192092896}], 'cnn3': [{'text': 'We first report the configuration which achieved the highest accuracy, followed by the search through the space of hyperparameters (using only our validation data set) leading to the optimal configuration. We close by demonstrating the ability of FF trained CNNs to implement Class Activation Maps, which is a method from the explainable AI toolbox.', 'similarity': 0.8359439373016357}], 'cnn4': [{'text': 'Convolutional Neural Networks (CNNs) have revolutionized the field of deep learning, especially in processing grid-like data structures such as images [1]. Their effectiveness in tasks like image classification [2, 3], object detection [4, 5], semantic segmentation [6, 7] and image generation [8] stem from their ability to effectively learn spatial features. Convolutional layers, using filters or kernels, capture local patterns and extract features from input images. One important feature of convolutional layers is the shared weights implemented by kernels. This allows for efficient deep learning on images, as using only fully connected layers for such tasks would result in unfathomable numbers of parameters. Pooling layers, like max pooling and average pooling, reduce the spatial dimensions of these features, helping the network to focus on the most significant aspects.\n\nDespite their popularity, CNNs face challenges in computational efficiency and adaptability. There have been several convolutional neural network architectures that have been proposed that are aimed at efficiency. Some of such architectures include MobileNet [12] and EfficientNet [13]. However, such traditional CNNs, with fixed architectures and number of parameters, may not perform uniformly across different types of input data with varying levels of complexity.\n\nNeural Architecture Search (NAS), a method for selecting optimal neural network architectures, has been a response to this challenge. NAS aims to obtain the best model for a specific task under certain constraints [14]. However, NAS is often resource-intensive due to the need to train multiple candidate models. in order to determine the optimal architecture. It is estimated that the carbon emission produced when using NAS to train a transformer model can amount to five times the lifetime carbon emissions of an average car [15]. This highlights the importance of finding suitable\n\narchitectures for neural networks, yet also points to the limitations of current approaches in terms of static structure and proneness to over-parameterization.\n\nSelf Expanding Neural Networks (SENN), introduced in [9], offer a promising direction. Inspired by neurogenesis, SENN dynamically adds neurons and fully connected layers to the architecture during training using a natural expansion score (defined in section 2.1) as a criteria to guide this process. This helps overcome the problem of over-parametrization. However, its application has been limited to multilayer perceptrons, with extensions to more practical architectures like CNNs identified as a future research prospect.\n\nOur study aims to develop a Self Expanding Convolutional Neural Network (SECNN), building on the concept of SENN and applying it to modern vision tasks. To the best of our knowledge, there has been no research on Self Expanding CNNs, despite the potential they hold for addressing model efficiency and adaptability in vision tasks. Unlike existing approaches that often require restarting training after modifications or rely on preset mechanisms for expansion, our approach utilizes the natural expansion score for dynamic and optimal model expansion. This research represents a significant step in developing adaptable, efficient CNN models for a variety of vision-related tasks.\n\nThe contributions of this research are as follows:\n\n- Developing a Self Expanding CNN that dynamically determines the optimal model size based on the task, thereby enhancing efficiency.\n- Eliminating the need to train multiple CNN models of varying sizes by allowing for the extraction of checkpoints at diverse complexity levels.\n- Eliminating the need to restart the training process after expanding the CNN model.', 'similarity': 0.8484948873519897}]}, 'Methodology': {'cnn1': [{'text': 'Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.\n\nThis paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.\n\nResearch in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.', 'similarity': 0.8373116850852966}], 'cnn2': [{'text': 'In this paper, we tried to analyze the properties of convolutional neural networks. A simplified model, the scattering transform was introduced as a first step towards understanding CNN operations. We saw that the feature transformation is built on top of wavelet transforms which separate variations at different scales using a wavelet transform. The analysis of general CNN architectures was not considered in this paper, but even this analysis is only a first step towards a full mathematical understanding of convolutional neural networks.', 'similarity': 0.8467366695404053}], 'cnn3': [{'text': 'This section will first discuss our new technique for labeling the positive and negative data sets, before explaining our implementation of the FF algorithm in detail.', 'similarity': 0.8417863249778748}], 'cnn4': [{'text': 'In order to develop a dynamically expanding convolutional neural-network architecture, we need an expansion criteria that triggers when to expand the model. The criteria we use is the natural expansion score as defined in section 2.1.', 'similarity': 0.9301211833953857}]}, 'Results': {'cnn1': [{'text': '1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642–3649. IEEE (2012)\n\n2. Cireşan, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2013, pp. 411–418. Springer (2013)\n\n3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\n\nIntroduction to Convolutional Neural Networks         11\n\n4. Cireşan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE (2011)\n\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networks a review. Pattern recognition 35(10), 2279–2301 (2002)\n\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257–260. IEEE (2010)\n\n7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\n\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\n\n9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221–231 (2013)\n\n10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725–1732. IEEE (2014)\n\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097–1105 (2012)\n\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541–551 (1989)\n\n13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n\n14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685–696 (1998)\n\n15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\n\n16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\n\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224–229. IEEE (2005)\n\n18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553–2561 (2013)\n\n19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157–2162. IEEE (2003)\n\n20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\n\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision–ECCV 2014, pp. 818–833. Springer (2014)', 'similarity': 0.8521066904067993}], 'cnn2': [{'text': '[1] Joakim Andén and Stéphane Mallat. Deep scattering spectrum. Signal Processing, IEEE Transactions on, 62(16):4114–4128, 2014.\n\n[2] Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1872–1886, 2013.\n\n[3] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3): 273–297, 1995.\n\n[4] Joan Bruna Estrach. Scattering representations for recognition.\n\n[5] Kaiser Gerald. A friendly guide to wavelets, 1994.\n\n[6] B Boser Le Cun, John S Denker, D Henderson, Richard E Howard, W Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems. Citeseer, 1990.\n\n[7] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\n[8] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.\n\n[9] Stéphane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):1331–1398, 2012.\n\n[10] Stéphane Mallat. Understanding deep convolutional networks. arXiv preprint arXiv:1601.04920, 2016.', 'similarity': 0.8521066904067993}], 'cnn3': [{'text': 'We first report the configuration which achieved the highest accuracy, followed by the search through the space of hyperparameters (using only our validation data set) leading to the optimal configuration. We close by demonstrating the ability of FF trained CNNs to implement Class Activation Maps, which is a method from the explainable AI toolbox.', 'similarity': 0.9046024084091187}], 'cnn4': [{'text': 'The results of our trials can be found in Figure 3 and a comparison of our model with other models on CIFAR-10 image classification can be found in Figure 4.\n\n| Metric | Trial 1 | Trial 2 | Trial 3 | NumberofParametersTrial 4 | Trial 5 | Mean |\n| - | - | - | - | - | - | - |\n| Val Accuracy (at 70%) | 13696 | 11360 | 11360 | 11360 | 9024 | 11360.0 |\n| Val Accuracy (at 80%) | 27852 | 51880 | 22636 | 37320 | 28588 | 33655.2 |\n| Highest Val Accuracy (%) | 83.4 | 84.6 | 84.6 | 84.5 | 83.2 | 84.1 |\n| Parameters at Highest Accuracy | 74564 | 73720 | 57808 | 66440 | 40960 | 62698.4 |\n\n\nFigure 3: Table displaying the number of parameters required to achieve different validation accuracies on CIFAR-10 over 5 different trials with the same hyperparameters.', 'similarity': 0.9121243953704834}]}}, 'content_results': {'cnn3': [{'text': 'The recent successes in analyzing images with deep neural networks are almost exclusively achieved with Convolutional Neural Networks (CNNs). The training of these CNNs, and in fact of all deep neural network architectures, uses the backpropagation algorithm, where the output of the network is compared with the desired result, and the difference is then used to tune the weights of the network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton suggested an alternative way of training which passes the desired results together with the images at the input of the network. This so called Forward-Forward (FF) algorithm has up to now only been used in fully connected networks. In this paper, we show how the FF paradigm can be extended to CNNs. Our FF-trained CNN, featuring a novel spatially-extended labeling technique, achieves a classification accuracy of 99.16% on the MNIST hand-written digits data set. We show how different hyperparameters affect the performance of the proposed algorithm and compare the results with CNNs trained with the standard backpropagation approach. Furthermore, we demonstrate that Class Activation Maps can be used to investigate which type of features are learnt by the FF algorithm.\n\n**Keywords:** forward-forward, backpropagation, convolutional neural networks, class activation maps, MNIST classification.\n\n1. * corresponding authors\n2. † equal contribution\n\n©20xx Scodellaro, Kulkarni, Alves and Schröter.\n\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/vxx/21-0000.html.\n\nScodellaro, Kulkarni, Alves and Schröter\n\n\nMachine learning using deep neural networks (DNN) continues to transform human life in areas as different as art (DALL-E, stable diffusion), medicine (Alpha-Fold), transport (self-driving cars, any time now), or information retrieval (ChatGPT, Bard). Here, the adjective deep refers to the number of layers of artificial neurons, which can go up to the hundreds. Training these networks means shifting the weights connecting the layers from their initial, random values to values which produce the correct predictions at the output layer of the DNN. This is achieved with the help of a loss function that computes the aggregate difference between the predicted output and the accurate results, which must be known for the training examples. The algorithm behind the training is some variant of gradient descent: in each round of training each weight is shifted a bit into the direction minimizing the loss by using the derivative of the loss function with respect to that weight. Taking the derivative of a loss function with respect to a given weight is straightforward for a single, output layer. Training the weights of the earlier layers in DNNs requires the gradient computation to be performed iteratively by applying the chain rule (Rumelhart et al., 1986). This process is called backpropagation (BP). Due to its importance, the term BP is also often used loosely to refer to the entire learning algorithm including the gradient descent.\n\nBackpropagation, respectively multi-layer gradient descent, has a number of downsides: first, it requires the storage of intermediate results. Depending on the optimizer, the memory consumption of BP is up to 5 times larger than the requirement for storing the weights alone (Hugging Face Community, 2023). This becomes a problem when training large models on GPU cards with limited memory. Second, under the name neuromorphic computing, there is an ongoing search of hardware alternatives to CMOS semiconductors, driven by the desire to decrease power consumption and increase processing speed (Christensen et al., 2022). On these new hardware platforms, it is often impossible to implement an analog of BP, raising the need for alternative training algorithm. Finally, evolution has clearly developed learning algorithms for neural networks such as our brain. However, those algorithms seem to be quite (but maybe not completely, see e.g. Lillicrap et al., 2020) different from BP. Given the in general high performance of evolutionary solutions to problems, this raises the question if deep learning could also gain from biologically plausible alternatives to BP.\n\nDue to these limitations, there is an ongoing search for alternative training methods. The most radical approach is to abandon the loss function completely and use a new learning paradigm. Neural networks trained with variants of the locally acting Hebbian learning rule (neurons that fire together wire together) have been shown to be competitive with BP (Journé et al., 2023; Zhou, 2022). Another approach, Equilibrium Propagation (Scellier and Bengio, 2017) is a learning framework for energy based models with symmetric connections between neurons. Layer-wise Feedback Propagation (Weber et al., 2023) removes the need for a gradient computation by replacing the objective of reducing the loss with computing a reward signal from the network output, and propagating that signal backward into the net. In contrast, the most conservative approach is to keep gradient descent, but to replace the required gradients with an estimate computed from the difference in loss of two forward passes with slightly modified weights. While the naive version of this approach, labeled zeroth order optimization, can be expected to be extremely inefficient, modern variants\n\nTRAINING CONVOLUTIONAL NEURAL NETWORKS WITH THE FORWARD-FORWARD ALGORITHM\n\nseem to be competitive (Baydin et al., 2022; McCaughan et al., 2023; Malladi et al., 2023; ?).\n\nA third category of algorithms maintains the idea to update the weights using derivatives of some signal, which involves the difference between the present state of the network and the target state. But it relaxes the requirement to backpropagate that signal from the output layer towards the earlier layers. This can either be done by using exclusively an output-derived error signal for training each intermediate layer (Nøkland, 2016; Flügel et al., 2023), or by training each layer with locally available information gathered in two consecutive forward passes. An example of the latter is the "Present the Error to Perturb the Input To modulate Activity technique" (PEPITA), which performs the second forward pass with the sum of the input signal used in the first pass and some random projection of the error signal from that pass (Dellaferrera and Kreiman, 2022; Farinha et al., 2023).\n\nAnother example of the use of local information gathered in consecutive runs to update the weights is the Forward-Forward (FF) algorithm (Hinton (2022)) proposed by Geoffrey Hinton in December 2022. FF training combines two ideas. First, the weights of a given layer are updated using gradients of a locally defined goodness function, which in Hinton (2022) is taken to be the sum of the squares of the activities in that layer. Second, the labels are included in the training data, which allows the neurons to learn them together. In order to understand which features of the data vote for a given label, half of the data set is comprised of labels combined with wrong images. For this negative data the weights are changed in order to minimize the goodness. In contrast, for the correctly labeled, positive data the weights are modified to maximize the goodness. Both these objectives can be achieved with a local gradient descent, with no need for BP. The term Forward-Forward now refers to having two subsequent training steps, one with positive and one with negative data.\n\nWhen generalizing this training method to multi-layer networks, it is important to assure that each subsequent layer needs to do more than just measure the length of the activity vector of the previous one. This is achieved by using layer normalization (Ba et al., 2016) which normalizes the activity vector for each sample. This is best summarized by the description in Hinton (2022): the activity vector in the first hidden layer has a length and an orientation. The length is used to define the goodness for that layer and only the orientation is passed to the next layer. There are two ways of using a FF trained network for inference. First, we can simultaneously train a linear classifier using the activities of the neurons in the different layers as input. Alternatively, we can create multiple copies of the data set under consideration and combine each copy with one of the possible labels. The correct label is then the one with the largest goodness during its forward pass. Note that this approach multiplies the amount of computation required for inference with a factor equal to the number of labels.\n\nGiven the repute of the proposer, it is not surprising that the Forward-Forward algorithm has inspired a number of groups to suggest modified and adapted versions. Examples include the combination with a generative model (Ororbia and Mali, 2023), multiple convolutional blocks (not trained with FF) (?), or extending FF training to graph neural networks (Paliotta et al., 2023) and spiking neural networks (Ororbia, 2023). In accordance with the neuromorphic computation motivation of moving beyond BP, the FF algorithm has also been used to train optical neural networks (Oguz et al., 2023) and microcontroller units\n\nScodellaro, Kulkarni, Alves and Schröter\n\nwith low computational resources (De Vita et al., 2023). Another line of research tries to improve FF by modifying how the goodness is computed (Lee and Song, 2023; Lorberbom et al., 2023; Gandhi et al., 2023), by understanding the sparsity of activation in FF trained networks (Tosato et al., 2023; Yang, 2023), or by exploring the capabilities of FF for self-supervised learning (Brenig and Timofte, 2023). There has been less activity in terms of practical applications of the FF algorithm. Particularly, in the classification of real world images it has been noted that FF performs worse than BP (Reyes-Angulo and Paheding, 2023) or has to be combined with BP to achieve satisfying results (Paheding and Reyes-Angulo, 2023). We propose that this lack of applications is due to the absence of a method to train the mainstay of modern image processing, Convolutional Neural Networks (CNN) (Rawat and Wang, 2017), with the FF algorithm. The ideas presented here close this gap.\n\nThis paper is structured as following: in Section 2 we will first introduce our spatial-extended labeling technique, which is crucial to preserve the label information during convolution. Then we will discuss the details of our implementation of the FF-based learning and inference. The results in Section 3 start with a discussion of the optimal results we obtained on the MNIST hand written digits classification. In Section 3.2, we describe our search for the optimal hyperparameters (using validation data). Section 3.3 shows how Class Activation Maps (CAMs) can be used to get a better understanding of the features learned during training. We close in Section 4 with a short discussion.', 'image': 'No image available', 'sub_heading': '', 'collection_name': 'cnn3', 'similarity': 0.8341025114059448}], 'cnn4': [{'text': 'Convolutional Neural Networks (CNNs) have revolutionized the field of deep learning, especially in processing grid-like data structures such as images [1]. Their effectiveness in tasks like image classification [2, 3], object detection [4, 5], semantic segmentation [6, 7] and image generation [8] stem from their ability to effectively learn spatial features. Convolutional layers, using filters or kernels, capture local patterns and extract features from input images. One important feature of convolutional layers is the shared weights implemented by kernels. This allows for efficient deep learning on images, as using only fully connected layers for such tasks would result in unfathomable numbers of parameters. Pooling layers, like max pooling and average pooling, reduce the spatial dimensions of these features, helping the network to focus on the most significant aspects.\n\nDespite their popularity, CNNs face challenges in computational efficiency and adaptability. There have been several convolutional neural network architectures that have been proposed that are aimed at efficiency. Some of such architectures include MobileNet [12] and EfficientNet [13]. However, such traditional CNNs, with fixed architectures and number of parameters, may not perform uniformly across different types of input data with varying levels of complexity.\n\nNeural Architecture Search (NAS), a method for selecting optimal neural network architectures, has been a response to this challenge. NAS aims to obtain the best model for a specific task under certain constraints [14]. However, NAS is often resource-intensive due to the need to train multiple candidate models. in order to determine the optimal architecture. It is estimated that the carbon emission produced when using NAS to train a transformer model can amount to five times the lifetime carbon emissions of an average car [15]. This highlights the importance of finding suitable\n\narchitectures for neural networks, yet also points to the limitations of current approaches in terms of static structure and proneness to over-parameterization.\n\nSelf Expanding Neural Networks (SENN), introduced in [9], offer a promising direction. Inspired by neurogenesis, SENN dynamically adds neurons and fully connected layers to the architecture during training using a natural expansion score (defined in section 2.1) as a criteria to guide this process. This helps overcome the problem of over-parametrization. However, its application has been limited to multilayer perceptrons, with extensions to more practical architectures like CNNs identified as a future research prospect.\n\nOur study aims to develop a Self Expanding Convolutional Neural Network (SECNN), building on the concept of SENN and applying it to modern vision tasks. To the best of our knowledge, there has been no research on Self Expanding CNNs, despite the potential they hold for addressing model efficiency and adaptability in vision tasks. Unlike existing approaches that often require restarting training after modifications or rely on preset mechanisms for expansion, our approach utilizes the natural expansion score for dynamic and optimal model expansion. This research represents a significant step in developing adaptable, efficient CNN models for a variety of vision-related tasks.\n\nThe contributions of this research are as follows:\n\n- Developing a Self Expanding CNN that dynamically determines the optimal model size based on the task, thereby enhancing efficiency.\n- Eliminating the need to train multiple CNN models of varying sizes by allowing for the extraction of checkpoints at diverse complexity levels.\n- Eliminating the need to restart the training process after expanding the CNN model.', 'image': 'No image available', 'sub_heading': '1 Introduction', 'collection_name': 'cnn4', 'similarity': 0.828028678894043}], 'cnn1': [{'text': 'Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.\n\nThis paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.\n\nResearch in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.', 'image': 'No image available', 'sub_heading': '4 Conclusion', 'collection_name': 'cnn1', 'similarity': 0.834538459777832}], 'cnn2': [{'text': ' Figure 1: Architecture of a Convolutional Neural Network (from LeCun et al. [7]) ', 'image': 'C:\\Users\\VICKY\\Desktop\\New folder\\output_directory\\cnn2\\image94-page3.png', 'sub_heading': '', 'collection_name': 'cnn2', 'similarity': 0.8447548151016235}]}}